{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for model development\n",
    "This notebook should be used to develop the model submodules locally. Note, that model relevant changes have to be made in the model subclasses. Additional code in this document is only used to get a better understanding of the dataflow and debug some outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: Set up environment\n",
    "### Install dependencies\n",
    "This will use the automation `install_dependencies.sh` in order to install necessary packages. Additional packages for development can be installed afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2: Get the data ready\n",
    "In this step, the data will be imported and preprocessed.\n",
    "\n",
    "### Download Dataset\n",
    "The dataset has to be imported using the submodule `model/import_data.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from create_dataset import create_dataset\n",
    "from helpers.s3 import enable_local_dev as s3_enable_local_dev\n",
    "from helpers.influx import enable_local_dev as influx_enable_local_dev\n",
    "\n",
    "# Enable local development (use local path instead of internal docker path)\n",
    "s3_enable_local_dev()\n",
    "influx_enable_local_dev()\n",
    "\n",
    "measurements = ['env_beige_default']\n",
    "img_size = (None, 75, 100, 3)\n",
    "\n",
    "dataset_id = \"5e1530ab-ba18-42fa-8897-590f6ea833e5\"\n",
    "images, labels, uids = create_dataset(measurements, img_size)\n",
    "max_input_value = 350\n",
    "\n",
    "del dataset_id\n",
    "_ = gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of images: \", images.shape[0])\n",
    "print(\"Labels from \", np.min(labels), \" to \", np.max(labels), \"\\n\")\n",
    "\n",
    "def plot_image(image, label):\n",
    "  plt.imshow(image)\n",
    "  plt.title(label)\n",
    "  plt.show()\n",
    "\n",
    "# Plot some images\n",
    "num_images_to_plot = 1\n",
    "for i in range(0, num_images_to_plot):\n",
    "  plot_image(images[i], f\"distance: {labels[i]}\")\n",
    "\n",
    "# Plot histogram of labels\n",
    "plt.hist(labels, edgecolor='black')  # Erstelle das Histogramm mit 10 Bins\n",
    "plt.title('Distribution of labels in dataset')\n",
    "plt.xlabel('Distance [mm]')\n",
    "plt.ylabel('n')\n",
    "plt.show()\n",
    "\n",
    "del num_images_to_plot, i\n",
    "_ = gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess dataset\n",
    "The dataset is preprocessed using the submodule `model/preprocess_data.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess_data import preprocess_data\n",
    "\n",
    "test_split = 0.2\n",
    "random_state = 0\n",
    "train_x, train_y, train_uids, test_x, test_y, test_uids = \\\n",
    "    preprocess_data(images, labels, uids, test_split, random_state)\n",
    "\n",
    "del images, labels, uids\n",
    "_ = gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTrainingsdaten: \")\n",
    "mean = np.mean(train_x)\n",
    "std = np.std(train_x)\n",
    "max = np.max(train_x)\n",
    "min = np.min(train_x)\n",
    "print(f\"Anzahl: {train_x.shape[0]}\")\n",
    "print(f\"Durchschnittlicher Pixelwert: {mean:.2f}\")\n",
    "print(f\"Standardabweichung der Pixelwerte: {std:.2f}\")\n",
    "print(f\"Höchster Pixelwert: {max:.2f}\")\n",
    "print(f\"Niedrigster Pixelwert: {min:.2f}\")\n",
    "print(f\"Bildgröße (Höhe x Breite): {train_x.shape[1]} x {train_x.shape[2]}\")\n",
    "print(f\"Min Label: {np.min(train_y)}\")\n",
    "print(f\"Max Label: {np.max(train_y)}\")\n",
    "print(f\"Verschiedene Labels: {len(np.unique(train_y))}\")\n",
    "\n",
    "plot_image(train_x[0], f\"distance: {train_y[0]}\")\n",
    "\n",
    "print(\"\\nTestdaten: \")\n",
    "mean = np.mean(test_x)\n",
    "std = np.std(test_x)\n",
    "max = np.max(test_x)\n",
    "min = np.min(test_x)\n",
    "print(f\"Anzahl: {test_x.shape[0]}\")\n",
    "print(f\"Durchschnittlicher Pixelwert: {mean:.2f}\")\n",
    "print(f\"Standardabweichung der Pixelwerte: {std:.2f}\")\n",
    "print(f\"Höchster Pixelwert: {max:.2f}\")\n",
    "print(f\"Niedrigster Pixelwert: {min:.2f}\")\n",
    "print(f\"Bildgröße (Höhe x Breite): {test_x.shape[1]} x {test_x.shape[2]}\")\n",
    "print(f\"Min Label: {np.min(test_y)}\")\n",
    "print(f\"Max Label: {np.max(test_y)}\")\n",
    "print(f\"Verschiedene Labels: {len(np.unique(test_y))}\")\n",
    "\n",
    "plot_image(test_x[0], f\"distance: {test_y[0]}\")\n",
    "\n",
    "# Plot histogram of labels\n",
    "print(\"\\nLabel distribution in training and test set:\")\n",
    "plt.hist([train_y, test_y], label=['Train', 'Test'],\n",
    "          edgecolor='black', color=['blue', 'orange'])\n",
    "plt.title('Distribution of labels in dataset')\n",
    "plt.xlabel('Distance [mm]')\n",
    "plt.ylabel('n')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "del mean, min, max, std\n",
    "_ = gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3: Model\n",
    "\n",
    "### Create model\n",
    "The model is created using `model/create_model.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from create_model import create_model\n",
    "\n",
    "model = create_model(test_x.shape, dropout=0.2, use_variation=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional: Plot model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model  # type: ignore\n",
    "plot_model(model, show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training\n",
    "The model is trained using `model_fit_model.py`\n",
    "This should ideally be done using the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fit_model import fit_model\n",
    "\n",
    "history = fit_model(\n",
    "              model, train_x, train_y,\n",
    "              optimizer='adam',\n",
    "              loss=\"mean_squared_error\",\n",
    "              metrics=['mae'],\n",
    "              epochs=50,\n",
    "              batch_size=128\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Final loss: {history.history['loss'][-1]:.4f}\")\n",
    "print(f\"Final accuracy: {history.history['mae'][-1]:.4f}\")\n",
    "\n",
    "print(history.history)\n",
    "\n",
    "# Plot training loss\n",
    "plt.figure(1)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epopch')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "# Plot training accuracy\n",
    "plt.figure(2)\n",
    "plt.plot(history.history['mae'])\n",
    "plt.title('Training Mean Average Error')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MAE')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model\n",
    "The model is evaluated using `model/evaluate_model.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate_model import evaluate_model\n",
    "\n",
    "test_loss, test_mae= evaluate_model(model, test_x, test_y)\n",
    "\n",
    "print(f'Test loss: {test_loss:.4f}')\n",
    "print(f'Test accuracy: {test_mae:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform random predictions on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "i = randint(0, test_x.shape[0])\n",
    "img = test_x[i:i+1]\n",
    "prediction = model.predict(img)\n",
    "prediction_transformed = int(prediction[0][0]*max_input_value)\n",
    "label_transformed = int(test_y[i]*max_input_value)\n",
    "print(\"Prediction:\", prediction_transformed)\n",
    "print(\"Label:\", label_transformed)\n",
    "\n",
    "plot_image(test_x[i], f\"Prediction: {prediction_transformed}, Label: {label_transformed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get predictions with difference to label > threshold [mm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "threshold = 20\n",
    "\n",
    "\n",
    "def predictOnDataset(X):\n",
    "    print(\"Predicting dataset, please wait ...\\n\")\n",
    "    predictions = np.zeros((len(X)), dtype=float)\n",
    "    for i in tqdm(range(X.shape[0]), desc=\"Progress: \", ncols=100, bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} images predicted'):\n",
    "        prediction = model.predict(X[i:i+1], verbose = 0)\n",
    "        predictions[i] = prediction[0][0]\n",
    "    print(\"\\nPredicted labels von \", np.min(predictions), \" bis \", np.max(predictions))\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def getOutliers(labels, predictions, threshold):\n",
    "# Filter predictions wich are to far away from the labels\n",
    "    outlier_image_idices = np.array([], dtype=np.int32)\n",
    "    for i, label in enumerate(labels):\n",
    "        if abs(predictions[i] - label) > threshold:\n",
    "            outlier_image_idices = np.append(outlier_image_idices, i)\n",
    "    print(\"Number of outliers: \", outlier_image_idices.shape[0], \" = \", len(outlier_image_idices)/len(labels)*100, \"%\")\n",
    "\n",
    "    return outlier_image_idices\n",
    "\n",
    "\n",
    "predictions = predictOnDataset(test_x)\n",
    "predictions_mm = (predictions*max_input_value).astype(int)\n",
    "labels_mm = (test_y*max_input_value).astype(int)\n",
    "outliers = getOutliers(labels_mm, predictions_mm, threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot outlier predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for outlier in outliers:\n",
    "    plot_image(test_x[outlier], f\"Prediction: {predictions_mm[outlier]}, Label: {labels_mm[outlier]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model.keras')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
